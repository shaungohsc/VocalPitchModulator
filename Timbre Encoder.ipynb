{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timbre Encoder\n",
    "This is the notebook used to train the Vocal Pitch Modulator.\n",
    "\n",
    "This notebook makes use of the data to train our timbre encoder.\n",
    "\n",
    "There are two models here, the first is a vowel classifier, that takes in an MFCC and outputs a vowel, and the second is a VAE that takes in an MFCC, reduces its dimensionality, and attempts to reconstruct the provided MFCC.\n",
    "\n",
    "## Global variables/Imports\n",
    "Run these cells before running either of the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import scipy.io as sio\n",
    "from scipy.io import wavfile\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "%aimport VPM\n",
    "from VPM import *\n",
    "%aimport Utils\n",
    "from Utils import *\n",
    "%aimport ANN\n",
    "from ANN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants that should not change without the dataset being changed\n",
    "n_pitches = 16\n",
    "n_vowels = 12\n",
    "n_people = 3\n",
    "\n",
    "# These dictionaries are more for reference than anything\n",
    "label_to_vowel = { 0: \"bed\",  1: \"bird\",   2: \"boat\",  3: \"book\", \n",
    "                   4: \"cat\",  5: \"dog\",    6: \"feet\",  7: \"law\",  \n",
    "                   8: \"moo\",  9: \"nut\",   10: \"pig\",  11: \"say\" }\n",
    "\n",
    "vowel_to_label = { \"bed\": 0,  \"bird\": 1,  \"boat\":  2, \"book\":  3,\n",
    "                   \"cat\": 4,  \"dog\":  5,  \"feet\":  6, \"law\":   7,\n",
    "                   \"moo\": 8,  \"nut\":  9,  \"pig\":  10, \"say\":  11}\n",
    "\n",
    "noteidx_to_pitch = {  0: \"A2\",   1: \"Bb2\",  2: \"B2\",   3: \"C3\",\n",
    "                      4: \"Db3\",  5: \"D3\",   6: \"Eb3\",  7: \"E3\", \n",
    "                      8: \"F3\",   9: \"Gb3\", 10: \"G3\",  11: \"Ab3\",\n",
    "                     12: \"A3\",  13: \"Bb3\", 14: \"B3\",  15: \"C4\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "Used to tune the data generation and ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 128\n",
    "n_mfcc = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "This is all the code that was explained in the Data Walkthrough. It generates data structures to hold all wav file data, spectrograms, mel spectra and MFCC data for all wav files.\n",
    "\n",
    "For diagram-visualization of the data set, refer to the [readme](https://github.com/zioul123/VocalPitchModulator/blob/master/README.md).\n",
    "\n",
    "For the classifier, MFCC are normalized by row to `[-1, 1]`, as the classifer can learn it better.\n",
    "For the VAE, MFCC are normalized by row to `[0, 1]`, to allow the VAE to output it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File reference lists\n",
    "data_ref_list = create_data_ref_list(os.path.join(\"Data\", 'dataset_files.csv'),\n",
    "                            n_pitches, n_vowels, n_people)\n",
    "# flat_data_ref_list[flat_ref_idx(vowel, pitch, person)]\n",
    "flat_data_ref_list = flatten_3d_array(data_ref_list, \n",
    "                                      n_vowels, n_pitches, n_people)\n",
    "\n",
    "# File reference list accessors\n",
    "# Returns a flat_ref_idx, given a vowel, pitch, person\n",
    "flat_ref_idx = lambda vowel, pitch, person: flat_3d_array_idx(\n",
    "    vowel, pitch, person, n_vowels, n_pitches, n_people)\n",
    "# Returns vowel, pitch, person, given a flat_ref_idx\n",
    "nd_ref_idx = lambda idx: nd_array_idx(idx, n_vowels, n_pitches, n_people)\n",
    "\n",
    "# Data-label pairs for pitch-shift training - not used here\n",
    "# data_label_pairs, _ = create_data_label_pairs(n_pitches)\n",
    "\n",
    "# wav, spectrogram, mels, mfcc for each file in flat_data_ref_list\n",
    "# wav_data:     (576, ~29400)  (n_wavs, n_samples)\n",
    "# spectrograms: (576, 513, 58) (n_wavs, n_freq_bins, n_windows)\n",
    "# mels:         (576, 128, 58) (n_wavs, n_mels, n_windows)\n",
    "# mfccs:        (576, 20, 58)  (n_wavs, n_mfcc, n_windows)\n",
    "all_wav_data = load_wav_files(os.path.join(\"Data\", \"dataset\"), \n",
    "                              flat_data_ref_list)\n",
    "all_spectrograms = np.array([ stft(waveform, plot=False) \n",
    "                              for waveform in all_wav_data ])\n",
    "all_mels, all_mfcc = map(np.array, map(list, zip(*\n",
    "                         [ ffts_to_mel(ffts, n_mels = n_mels, n_mfcc = n_mfcc) \n",
    "                           for ffts in all_spectrograms ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment one of these lines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize each mfcc (i.e. rows of the all_mfcc array) to [-1,1] - USE FOR THE CLASSIFER (TimbreEncoder)\n",
    "# all_mfcc = normalize_rows(all_mfcc, NormMode.REAL_TO_NEG_ONE_ONE)\n",
    "#Normalize each mfcc (i.e. rows of the all_mfcc array) to [0,1] - USE FOR THE VAE (TimbreVAE)\n",
    "all_mfcc = normalize_rows(all_mfcc, NormMode.REAL_TO_ZERO_ONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Label Structuring\n",
    "This puts together the actual data-label pairs to be fed into the ANN.\n",
    "\n",
    "Generate `data` and `labels` from `all_mfcc` and using `nd_ref_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_files, n_mfcc_dummy, n_windows = all_mfcc.shape\n",
    "\n",
    "# vowel_labels: (576) (n_wavs)\n",
    "all_vowel_labels, _, _ = map(np.array, map(list, zip(*\n",
    "                         [ nd_ref_idx(idx) \n",
    "                           for idx in range(len(flat_data_ref_list)) ])))\n",
    "\n",
    "# Returns a flat 2d idx, given a wavfile index and a window index\n",
    "flat_data_idx = lambda wav_idx, win_idx: flat_2d_array_idx(\n",
    "    wav_idx, win_idx, n_files, n_windows)\n",
    "\n",
    "# data:   (33408, 20) (n_wavs * n_windows, n_mfcc)\n",
    "# labels: (33408) (n_wavs * n_windows)\n",
    "data = np.array([ all_mfcc[wav_file_idx][:, window_idx] \n",
    "                  for wav_file_idx in range(n_files) \n",
    "                  for window_idx in range(n_windows) ])\n",
    "labels = np.array([ all_vowel_labels[wav_file_idx]\n",
    "                    for wav_file_idx in range(n_files)\n",
    "                    for window_idx in range(n_windows) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purposes - verify that the mfcc have been arranged in order of\n",
    "# wav_idx, win_idx, mel_feature_idx\n",
    "for wav_idx in range(n_files):\n",
    "    for win_idx in range(n_windows):\n",
    "        for m in range(n_mfcc_dummy):\n",
    "            assert data[flat_data_idx(wav_idx, win_idx)][m] == \\\n",
    "                   all_mfcc[wav_idx][m][win_idx]\n",
    "# Verify that the labels are arranged in order of wav_idx, win_idx\n",
    "for wav_idx in range(n_files):\n",
    "    for win_idx in range(n_windows):\n",
    "        assert labels[flat_data_idx(wav_idx, win_idx)] == \\\n",
    "               all_vowel_labels[wav_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data into `train` and `test`, and convert to Torch tensors of the correct types. Run **only one of these cells.**\n",
    "\n",
    "First method (**not-recommended**, simple): Random sampling to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, Y_train: (25056, 20) (25056) \n",
    "# X_val, Y_val:     (8352, 20) (8352)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data, labels, stratify=labels, random_state=0)\n",
    "X_train, Y_train, X_val, Y_val = map(torch.tensor, (X_train, Y_train, X_val, Y_val))\n",
    "# Default tensor is float\n",
    "X_train = X_train.float(); X_val = X_val.float()\n",
    "# Used as index, so it is long\n",
    "Y_train = Y_train.long(); Y_val = Y_val.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second method **(recommended)**: 1 person from each wav will be the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, Y_train: (22272, 20) (22272,) \n",
    "# X_val, Y_val:     (11136, 20) (11136,)\n",
    "X_train = []; X_val = []; Y_train = []; Y_val = []\n",
    "for vow_idx in range(n_vowels):\n",
    "    for pit_idx in range(n_pitches):\n",
    "        # Choose the person for this pitch/vowel to be used as test data\n",
    "        test_pid = int(np.random.rand() * 3)\n",
    "        for pid_idx in range(n_people):\n",
    "            wav_idx = flat_ref_idx(vow_idx, pit_idx, pid_idx)\n",
    "            if (pid_idx != test_pid):\n",
    "                for win_idx in range(n_windows):\n",
    "                    X_train.append(data[flat_data_idx(wav_idx, win_idx)])\n",
    "                    Y_train.append(labels[flat_data_idx(wav_idx, win_idx)])\n",
    "            else:\n",
    "                for win_idx in range(n_windows):\n",
    "                    X_val.append(data[flat_data_idx(wav_idx, win_idx)])\n",
    "                    Y_val.append(labels[flat_data_idx(wav_idx, win_idx)])  \n",
    "X_train, Y_train, X_val, Y_val = map(torch.tensor, (X_train, Y_train, X_val, Y_val))\n",
    "# Default tensor is float\n",
    "X_train = X_train.float(); X_val = X_val.float()\n",
    "# Used as index, so it is long\n",
    "Y_train = Y_train.long(); Y_val = Y_val.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timbre-Encoder - MFCC -> Vowel\n",
    "This takes MFCC (and mel-spectrograms in future?), and tries to identify the vowel spoken.\n",
    "\n",
    "**Results:**\n",
    "```\n",
    "| epochs | n_mfcc | n_hid | n_timb | Val acc |\n",
    "|   5000 |     20 |    12 |      4 | 0.77898 |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hid = 12; n_timb = 4; lr = 0.2; n_epochs = 5000;\n",
    "\n",
    "# Training model \n",
    "model = TimbreEncoder(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb, n_vowels=n_vowels)\n",
    "# Define loss \n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Available\" if torch.cuda.is_available() else \"GPU Not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if possible (will run on CPU otherwise)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Move inputs to GPU (if possible)\n",
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "Y_val = Y_val.to(device)\n",
    "\n",
    "# Move the network to GPU (if possible)\n",
    "model.to(device) \n",
    "# Define optimizer \n",
    "# opt = optim.SGD(model.parameters(), lr=lr)\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Fit the model\n",
    "tic = time.time()\n",
    "loss = model.train_func(X_train, Y_train, X_val, Y_val, model, opt,\n",
    "                        loss_fn, epochs=n_epochs, print_graph=True)\n",
    "toc = time.time()\n",
    "print('Final loss: {}\\nTime taken: {}'.format(loss, toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "model_path = os.path.join(\"model_data\", \"TimbreEncoder_{}_{}_{}_{}_{}_{}.pt\"\n",
    "                          .format(lr, n_epochs, n_mfcc, n_hid, n_timb, loss))\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Model saved at {}\".format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the saved model, and using the model for prediction (whole dataset) example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimbreEncoder(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb, n_vowels=n_vowels)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "# model.to(device)\n",
    "\n",
    "data_tensor, label_tensor = map(torch.tensor, (data, labels))\n",
    "data_tensor = data_tensor.float(); label_tensor = label_tensor.long(); \n",
    "# data_tensor = data_tensor.to(device); label_tensor = label_tensor.to(device)\n",
    "\n",
    "correct = 0; wrong = 0;\n",
    "corrects = np.zeros(n_vowels); wrongs = np.zeros(n_vowels)\n",
    "predictions = np.zeros((n_vowels, n_vowels));\n",
    "for vowel_idx in range(n_vowels):\n",
    "    for pitch_idx in range(n_pitches):\n",
    "        for pid_idx in range(n_people):\n",
    "            wav_idx = flat_ref_idx(vowel_idx, pitch_idx, pid_idx)\n",
    "            for win_idx in range(n_windows):\n",
    "                data_idx = flat_data_idx(wav_idx, win_idx)\n",
    "                label = (label_tensor[data_idx]).item()\n",
    "                pred = (torch.argmax(model(data_tensor[data_idx]))).item()\n",
    "                \n",
    "                predictions[vowel_idx][pred] = predictions[vowel_idx][pred] + 1\n",
    "                if label == pred:\n",
    "                    correct = correct + 1\n",
    "                    corrects[vowel_idx] = corrects[vowel_idx] + 1\n",
    "                else:\n",
    "                    wrong = wrong + 1\n",
    "                    wrongs[vowel_idx] = wrongs[vowel_idx] + 1\n",
    "                    \n",
    "print(\"Total Accuracy: {}\"\n",
    "      .format(correct / (wrong + correct)))\n",
    "for vowel_idx in range(n_vowels):\n",
    "    print(\"Vowel: {}. Accuracy: {}. Most common pred: {}\"\n",
    "          .format(label_to_vowel[vowel_idx],\n",
    "                  corrects[vowel_idx] / (wrongs[vowel_idx] + corrects[vowel_idx]),\n",
    "                  label_to_vowel[np.argmax(predictions[vowel_idx])]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timbre-VAE - MFCC -> MFCC\n",
    "This takes MFCC, reduces dimensionality to a `n_timb` latent space, and attempts to recreate the MFCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hid = 10; n_timb = 4; lr = 1e-3; n_epochs = 10000; batch_size=22272\n",
    "\n",
    "# Training model \n",
    "model = TimbreVAE(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb)\n",
    "\n",
    "# Define loss - from pytorch VAE example.\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available\" if torch.cuda.is_available() else \"GPU Not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7f62d8292e44b4bf4717df5c9634bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training', max=10000.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss before/after: 14.152862899604886, 12.745376762302442\n",
      "Validation Loss before/after: 14.133682426364942, 12.757947198275861\n",
      "Final loss: (12.745376762302442, 12.757944392061782)\n",
      "Time taken: 97.92482924461365\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if possible (will run on CPU otherwise)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Move inputs to GPU (if possible)\n",
    "X_train = X_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "\n",
    "# Move the network to GPU (if possible)\n",
    "model.to(device) \n",
    "# Define optimizer \n",
    "# opt = optim.SGD(model.parameters(), lr=lr)\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Fit the model\n",
    "tic = time.time()\n",
    "loss = model.train_func(X_train, X_val, model, opt, loss_fn, batch_size=batch_size,\n",
    "                        epochs=n_epochs, print_graph = True)\n",
    "toc = time.time()\n",
    "print('Final loss: {}\\nTime taken: {}'.format(loss, toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "en1.weight \t torch.Size([10, 20])\n",
      "en1.bias \t torch.Size([10])\n",
      "en_mu.weight \t torch.Size([4, 10])\n",
      "en_mu.bias \t torch.Size([4])\n",
      "en_std.weight \t torch.Size([4, 10])\n",
      "en_std.bias \t torch.Size([4])\n",
      "de1.weight \t torch.Size([10, 4])\n",
      "de1.bias \t torch.Size([10])\n",
      "de2.weight \t torch.Size([20, 10])\n",
      "de2.bias \t torch.Size([20])\n",
      "Model saved at model_data\\TimbreVAE_lr0.001_epochs10000_nmfcc20_nhid10_ntimb4_(12.745376762302442, 12.757944392061782).pt\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "model_path = os.path.join(\"model_data\", \"TimbreVAE_lr{}_epochs{}_nmfcc{}_nhid{}_ntimb{}_{}.pt\"\n",
    "                          .format(lr, n_epochs, n_mfcc, n_hid, n_timb, loss))\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Model saved at {}\".format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the saved model, and using the model for prediction example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.9751, 0.5035, 0.5754, 0.5136, 0.4108, 0.4069, 0.5212, 0.5884,\n",
      "        0.4402, 0.4514, 0.4929, 0.4635, 0.4682, 0.5167, 0.4709, 0.5345, 0.5320,\n",
      "        0.4620, 0.4920])\n",
      "tensor([0.0221, 0.9477, 0.4320, 0.5652, 0.5092, 0.4936, 0.4293, 0.4917, 0.5016,\n",
      "        0.4374, 0.4713, 0.4868, 0.4783, 0.4741, 0.4910, 0.4817, 0.4983, 0.4924,\n",
      "        0.4880, 0.4746], grad_fn=<SigmoidBackward>)\n",
      "tensor([-0.7843, -1.1944,  0.6204, -0.7380], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = TimbreVAE(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "data_tensor = torch.tensor(data)\n",
    "data_tensor = data_tensor.float();\n",
    "\n",
    "wav_idx = flat_ref_idx(5, 5, 1)\n",
    "data_idx = flat_data_idx(wav_idx, 30)\n",
    "label = data_tensor[data_idx]\n",
    "pred = model(data_tensor[data_idx])[0]\n",
    "latent = model.get_z(data_tensor[data_idx])\n",
    "\n",
    "print(label)\n",
    "print(pred)\n",
    "print(latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Choosing `n_hid`, `n_timb`\n",
    "\n",
    "From running this, we find the results:\n",
    "Best `n_hid`: 10, Best `n_timb`: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d296f7d35d4c74acf7f46dab5925ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 6, n_timb: 4', max=2500.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 6, n_timb: 4, Final val loss: 12.756245229436063, Time taken: 24.036638021469116\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_6_4_12.756245229436063.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637a70bd5180424cb248d25fc7782aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 8, n_timb: 4', max=2500.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 8, n_timb: 4, Final val loss: 12.756556719198993, Time taken: 23.402870178222656\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_8_4_12.756556719198993.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4631374efbbf4dd393a26627aaf5e0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 8, n_timb: 6', max=2500.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 8, n_timb: 6, Final val loss: 12.757583793552442, Time taken: 24.187214136123657\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_8_6_12.757583793552442.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1234f6522da7490f8fea6ba030324f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 10, n_timb: 4', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 10, n_timb: 4, Final val loss: 12.755354256465518, Time taken: 23.62875461578369\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_10_4_12.755354256465518.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffe0fb4c21e4813a77fe5e903c54b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 10, n_timb: 6', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 10, n_timb: 6, Final val loss: 12.756701239224139, Time taken: 23.57104468345642\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_10_6_12.756701239224139.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3ffe45c5104ad68621a431a255b167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 10, n_timb: 8', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 10, n_timb: 8, Final val loss: 12.75628732264727, Time taken: 23.87569832801819\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_10_8_12.75628732264727.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfdae241df14f52b49ad43e45a25f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 12, n_timb: 4', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 12, n_timb: 4, Final val loss: 12.75652725395115, Time taken: 23.377437353134155\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_12_4_12.75652725395115.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bdd35cf8da4354b09ed4e8ef78ad9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 12, n_timb: 6', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 12, n_timb: 6, Final val loss: 12.756516029094827, Time taken: 24.132755041122437\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_12_6_12.756516029094827.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80524de2ec654ee5acf77ad615f62c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 12, n_timb: 8', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 12, n_timb: 8, Final val loss: 12.757957020025144, Time taken: 21.46918272972107\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_12_8_12.757957020025144.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcd9d9a0a3343c9a7652928ae3ff05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 12, n_timb: 10', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 12, n_timb: 10, Final val loss: 12.757127783764368, Time taken: 24.31379532814026\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_12_10_12.757127783764368.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8c7d87690647749f976883fcee2983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 4', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 4, Final val loss: 12.75653426948635, Time taken: 23.120083808898926\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_14_4_12.75653426948635.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549a86014cec4551b236af68296f1858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 6', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 6, Final val loss: 12.75579904139727, Time taken: 23.299814462661743\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_14_6_12.75579904139727.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f86b8c40f744bbb90b1b531ebcb35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 8', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 8, Final val loss: 12.757596421515805, Time taken: 24.524491548538208\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_14_8_12.757596421515805.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277dd35e7e734f41aa39c29bc911df44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 10', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 10, Final val loss: 12.756250841864224, Time taken: 24.066789388656616\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_14_10_12.756250841864224.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddccc7066410466cbdd1f752cbcedcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 12', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 12, Final val loss: 12.757847577676007, Time taken: 23.379464149475098\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_14_12_12.757847577676007.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77a68e0d1ad433f9c96b54812956090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 4', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 4, Final val loss: 12.75629153196839, Time taken: 23.141919136047363\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_4_12.75629153196839.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29807120aed403587af8f2212b5cfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 6', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 6, Final val loss: 12.75677841011135, Time taken: 23.408835649490356\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_6_12.75677841011135.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbd525a318d4eb3858a38987e7fbc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 8', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 8, Final val loss: 12.757703057650861, Time taken: 23.205310583114624\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_8_12.757703057650861.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164ad8405d7141fc9240f6a1c2167747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 10', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 10, Final val loss: 12.756217167295258, Time taken: 18.973145723342896\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_10_12.756217167295258.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb355397ada040c6a7f8bf467ec87881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 12', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 12, Final val loss: 12.756746138649426, Time taken: 18.91083335876465\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_12_12.756746138649426.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6f9708fc6b43bea0f2924e35c104b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 14', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 14, Final val loss: 12.757397180316092, Time taken: 19.097800970077515\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_14_12.757397180316092.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b581abac8004f38b745c698ece54d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 4', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 4, Final val loss: 12.756252244971265, Time taken: 18.403200149536133\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_4_12.756252244971265.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5c5069a36242f096ed402dba273109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 6', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 6, Final val loss: 12.755765366828305, Time taken: 20.536202669143677\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_6_12.755765366828305.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8c0252adfb4320b68b49d0684bb304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 8', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 8, Final val loss: 12.756218570402298, Time taken: 18.79640245437622\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_8_12.756218570402298.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785fe56719e345f5b9911198c0e7a7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 10', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 10, Final val loss: 12.757287737966953, Time taken: 19.083486557006836\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_10_12.757287737966953.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69588899cbf04340b123a9f65a4a3c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 12', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 12, Final val loss: 12.757718491828305, Time taken: 19.04806160926819\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_12_12.757718491828305.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39afd0e0656a482c94401b5f7a4a7caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 14', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 14, Final val loss: 12.75823904454023, Time taken: 19.794686317443848\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_14_12.75823904454023.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b3e05805e848aa9c6decf75e2f9a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 16', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 16, Final val loss: 12.756389749461206, Time taken: 18.548084497451782\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_16_12.756389749461206.pt\n"
     ]
    }
   ],
   "source": [
    "# Define loss - from pytorch VAE example.\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "n_hid_candidates = [6, 8, 10, 12, 14, 16, 18]\n",
    "n_timb_candidates = [4, 6, 8, 10, 12, 14, 16]\n",
    "lr = 1e-3; n_epochs = 2500; batch_size=22272\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "losses = []\n",
    "for hid_idx in range(len(n_hid_candidates)):\n",
    "    for timb_idx in range(len(n_timb_candidates)):\n",
    "        if (hid_idx < timb_idx): continue\n",
    "        n_hid = n_hid_candidates[hid_idx]\n",
    "        n_timb = n_timb_candidates[timb_idx]\n",
    "        \n",
    "        # Training model \n",
    "        model = TimbreVAE(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb)\n",
    "\n",
    "        X_train = X_train.to(device)\n",
    "        X_val = X_val.to(device)\n",
    "        model.to(device) \n",
    "        # opt = optim.SGD(model.parameters(), lr=lr)\n",
    "        opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Fit the model\n",
    "        tic = time.time()\n",
    "        loss, val_loss = model.train_func(X_train, X_val, model, opt, loss_fn, batch_size=batch_size,\n",
    "                                epochs=n_epochs, print_graph=False, desc=\"n_hid: {}, n_timb: {}\".format(n_hid, n_timb))\n",
    "        toc = time.time()\n",
    "        print('n_hid: {}, n_timb: {}, Final val loss: {}, Time taken: {}'.format(n_hid, n_timb, val_loss, toc - tic))\n",
    "        model_path = os.path.join(\"model_data\", \"TimbreVAE_n_hid_n_timb_experiment_{}_{}_{}.pt\"\n",
    "                                  .format(n_hid, n_timb, val_loss))\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(\"Model saved at {}\".format(model_path)) \n",
    "        losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_hid: 10, Best n_timb: 4\n",
      "[[12.756245229436063 list([6, 4])]\n",
      " [12.756556719198993 list([8, 4])]\n",
      " [12.757583793552442 list([8, 6])]\n",
      " [12.755354256465518 list([10, 4])]\n",
      " [12.756701239224139 list([10, 6])]\n",
      " [12.75628732264727 list([10, 8])]\n",
      " [12.75652725395115 list([12, 4])]\n",
      " [12.756516029094827 list([12, 6])]\n",
      " [12.757957020025144 list([12, 8])]\n",
      " [12.757127783764368 list([12, 10])]\n",
      " [12.75653426948635 list([14, 4])]\n",
      " [12.75579904139727 list([14, 6])]\n",
      " [12.757596421515805 list([14, 8])]\n",
      " [12.756250841864224 list([14, 10])]\n",
      " [12.757847577676007 list([14, 12])]\n",
      " [12.75629153196839 list([16, 4])]\n",
      " [12.75677841011135 list([16, 6])]\n",
      " [12.757703057650861 list([16, 8])]\n",
      " [12.756217167295258 list([16, 10])]\n",
      " [12.756746138649426 list([16, 12])]\n",
      " [12.757397180316092 list([16, 14])]\n",
      " [12.756252244971265 list([18, 4])]\n",
      " [12.755765366828305 list([18, 6])]\n",
      " [12.756218570402298 list([18, 8])]\n",
      " [12.757287737966953 list([18, 10])]\n",
      " [12.757718491828305 list([18, 12])]\n",
      " [12.75823904454023 list([18, 14])]\n",
      " [12.756389749461206 list([18, 16])]]\n"
     ]
    }
   ],
   "source": [
    "indices = [ [n_hid_candidates[hid_idx], n_timb_candidates[timb_idx]] \n",
    "            for hid_idx in range(len(n_hid_candidates))\n",
    "            for timb_idx in range(len(n_timb_candidates)) if (hid_idx >= timb_idx) ]\n",
    "best_hid, best_timb = indices[np.argmin(np.array(losses))]\n",
    "print(\"Best n_hid: {}, Best n_timb: {}\".format(best_hid, best_timb))\n",
    "print(np.array(list(zip(losses, indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
